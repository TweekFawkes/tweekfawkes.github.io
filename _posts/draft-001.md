## AI LLM Jailbreak Research

- AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts
  - Nov 7, 2020
    - https://arxiv.org/pdf/2010.15980
    - https://arxiv.org/abs/2010.15980

---

- Black Box Adversarial Prompting for Foundation Models
  - Feb 8 2023
    - https://arxiv.org/pdf/2302.04237
    - https://arxiv.org/abs/2302.04237

---

- Universal and Transferable Adversarial Attacks on Aligned Language Models
  - Jul 27, 2023
    - https://arxiv.org/pdf/2307.15043
    - https://arxiv.org/abs/2307.15043
    - https://github.com/llm-attacks/llm-attacks/
    - https://llm-attacks.org/

---

- "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models
  - Aug 7 2023
    - https://arxiv.org/abs/2308.03825
    - https://github.com/verazuo/jailbreak_llms
    - https://jailbreak-llms.xinyueshen.me/

---

- Jailbreaking Black Box Large Language Models in Twenty Queries
  - October 12, 2023
    - https://arxiv.org/pdf/2310.08419
    - https://arxiv.org/abs/2310.08419

---
